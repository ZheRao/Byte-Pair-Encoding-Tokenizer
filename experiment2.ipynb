{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Optional\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"for invalid user configuration of tokenizer class\"\"\"\n",
    "    pass\n",
    "\n",
    "class base_tokenizer:\n",
    "    \"\"\"\n",
    "        This is the base class for tokenizer, it provde basic functionalities such as produce pair counts and merge new tokens\n",
    "        Args:\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self,title,vocab_size):\n",
    "        # initialize base vocabulary dictionary which is the character encoding based on UTF-8\n",
    "        self._base_vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self._base_vocab_size = 256\n",
    "        self._get_file_paths(title,vocab_size)\n",
    "    \n",
    "    def _get_file_paths(self,title,vocab_size):\n",
    "        self.folder = title + \"_tok_folder\"\n",
    "        self.vocab_path = os.path.join(self.folder, title + \"_vocab_dict_size\"+str(vocab_size)+\".pkl\")\n",
    "        self.merges_path = os.path.join(self.folder, title + \"_merge_history_size\"+str(vocab_size)+\".pkl\")\n",
    "        self.tokens_path = os.path.join(self.folder, title + \"_tokens_size\"+str(vocab_size)+\".npy\")\n",
    "\n",
    "    def _get_pair_counts(self,tokens):\n",
    "        \"\"\"\n",
    "            treverse through the entire encoded text, produce a dictionary with paired occurrences of adjacent tokens\n",
    "                key: token pairs, e.g., (106, 32)\n",
    "                value: counts of occurrence of key, e.g., 300\n",
    "                meaning: token pair (106, 32) occurred 300 times in the text\n",
    "        \"\"\"\n",
    "\n",
    "        count_dict = {}\n",
    "        for (c1, c2) in zip(tokens[:-1],tokens[1:]):\n",
    "            count_dict[(c1,c2)] = count_dict.get((c1,c2),0) + 1\n",
    "        return count_dict\n",
    "    \n",
    "    def _merge_pair(self,tokens,pair,new_token):\n",
    "        \"\"\"\n",
    "            Replace all occurrences of pair in tokens by new_token\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i],tokens[i+1]) == pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def _retrieve_training_history(self,title,vocab_size):\n",
    "        \"\"\"\n",
    "            retrieve the dictionaries\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.vocab_path,\"rb\") as f:\n",
    "                vocab = pickle.load(f)\n",
    "            with open(self.merges_path,\"rb\") as f:\n",
    "                merges = pickle.load(f)\n",
    "            with open(self.tokens_path,\"rb\") as f:\n",
    "                past_tokens = np.load(f)\n",
    "        except:\n",
    "            m = f\"Dictionary files do not exit, tokenizer requires training with {title}. Or provided with inconsistent vocab_size, use os.listdir to inspect dictionary files.\"\n",
    "            m_more = \" Or past_tokens cannot be retreived, if this is the case, encode text first\"\n",
    "            raise FileNotFoundError(m+m_more)\n",
    "        else:\n",
    "            return vocab, merges, past_tokens\n",
    "\n",
    "class TrainTokenizer(base_tokenizer):\n",
    "    \"\"\"\n",
    "        This class implement compression algorithm described in \n",
    "            https://en.wikipedia.org/wiki/Byte_pair_encoding#:~:text=Byte%20pair%20encoding%20(also%20known,for%20use%20in%20downstream%20modeling\n",
    "        It takes text and title, train a tokenizer and store the files in a directory\n",
    "        Args:\n",
    "            text: str, actual text\n",
    "            title: str, name of the mateirals that the tokenizer is training on\n",
    "            fresh_start: bool, whether to train from scratch or continue training/compressing, default=True\n",
    "            final_vocab_size: int, final vocabulary size after compression - determines how many merges to perform, in thousands\n",
    "            last_vocab_size: int, if continue training, what is the last final_vocab_size in thousands: 10 -> 10,000\n",
    "        \n",
    "        Folder/Title Naming Convention:\n",
    "            \"book_title_tok_folder\" - all lower case, connected with underscore\n",
    "        \n",
    "        Sub-files (in the _tok_folder) Naming Convetion:\n",
    "            \"book_title_vocab_dict_size10.pkl\" stores encoding dictionary, where 10 means 10,000 vocabulary size\n",
    "            \"book_title_merge_history_size10.pkl\" stores merging history, where 10 means 10,000 vocabulary size\n",
    "            \"book_title_tokens_size10.npy\" stores tokens from last compression, with tokenization with size 10,000\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, title: str, final_vocab_size: int =6000, fresh_start: bool =True, last_vocab_size: Optional[int] =None):\n",
    "        super(TrainTokenizer,self).__init__(title,last_vocab_size)\n",
    "        self.title = title\n",
    "        self.final_vocab_size = final_vocab_size\n",
    "        # initialize training vocabulary and merge history dictionaries\n",
    "        if fresh_start:\n",
    "            self.vocab = self._base_vocab\n",
    "            self.merge_history = {}\n",
    "            self.tokens = list(text.encode(\"utf-8\"))\n",
    "        else:\n",
    "            if last_vocab_size is None: raise ConfigError(\"for continue training (fresh_start == False), last final_vocab_size must be provided\")\n",
    "            if final_vocab_size <= last_vocab_size: raise ConfigError(\"unable to perform tokenizer training, because new vocabulary size must be larger than previous vocabulary size\")\n",
    "            self.vocab, self.merge_history, self.tokens = self._retrieve_training_history(self.title,last_vocab_size)\n",
    "        assert len(self.vocab) == (len(self.merge_history) + self._base_vocab_size), \"dictionary lengths not matching - the following should be true: voca = merge_hist + 256\"\n",
    "        assert final_vocab_size > len(self.vocab), f\"final vocabulary size specified is too small, must be larger than {len(self.vocab)}\"\n",
    "    \n",
    "    def run(self):\n",
    "        return self.vocab, self.merge_history, self.tokens\n",
    "    \n",
    "    \n",
    "    def _perform_merge(self):\n",
    "        \"\"\"\n",
    "            Training loop compression process:\n",
    "                1. identify top pair\n",
    "                2. swap the occurrences of top pair in the original tokens by new token\n",
    "                3. update merge_history and vocab\n",
    "            after training, vocab, merge_history are saved as pickle files and final tokens are saved as npy file\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.merge_history) + self._base_vocab_size\n",
    "        num_merges = self.final_vocab_size - vocab_size\n",
    "        for i in range(num_merges):\n",
    "            pair_counts = self._get_pair_counts(tokens=self.tokens)\n",
    "            top_pair = max(pair_counts,key=pair_counts.get)\n",
    "            self.tokens = self._merge_pair(tokens=self.tokens,pair=top_pair,new_token=vocab_size+i)\n",
    "            self.merge_history[top_pair] = vocab_size+i\n",
    "            self.vocab[vocab_size+i] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "            print(f\"merged {top_pair} as {vocab_size+i}\")\n",
    "        \n",
    "        self._get_file_paths(self.title,self.final_vocab_size)\n",
    "\n",
    "        if not os.path.exists(self.folder):\n",
    "            os.makedirs(self.folder)\n",
    "        \n",
    "        # save files to folder\n",
    "        with open(self.vocab_path,\"wb\") as f:\n",
    "            pickle.dump(self.vocab,f)\n",
    "        with open(self.merges_path,\"wb\") as f:\n",
    "            pickle.dump(self.merge_history, f)\n",
    "        with open(self.tokens_path,\"wb\") as f:\n",
    "            np.save(f,self.tokens)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length: 3729059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[84, 104, 101]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"lord-of-the-rings-processed.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "sample = text\n",
    "print(f\"text length: {len(sample)}\")\n",
    "tokens = list(sample.encode(\"utf-8\"))\n",
    "tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TrainTokenizer(text=sample,title=\"test\",final_vocab_size=280,fresh_start=False,last_vocab_size=260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged (101, 32) as 256\n",
      "merged (116, 104) as 257\n",
      "merged (100, 32) as 258\n",
      "merged (116, 32) as 259\n"
     ]
    }
   ],
   "source": [
    "tokenizer._perform_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged (115, 32) as 260\n",
      "merged (97, 110) as 261\n",
      "merged (105, 110) as 262\n",
      "merged (32, 257) as 263\n",
      "merged (101, 114) as 264\n",
      "merged (44, 32) as 265\n",
      "merged (46, 32) as 266\n",
      "merged (261, 258) as 267\n",
      "merged (111, 117) as 268\n",
      "merged (263, 256) as 269\n",
      "merged (111, 114) as 270\n",
      "merged (121, 32) as 271\n",
      "merged (101, 110) as 272\n",
      "merged (97, 114) as 273\n",
      "merged (111, 110) as 274\n",
      "merged (111, 102) as 275\n",
      "merged (101, 258) as 276\n",
      "merged (111, 32) as 277\n",
      "merged (262, 103) as 278\n",
      "merged (108, 108) as 279\n"
     ]
    }
   ],
   "source": [
    "tokenizer._perform_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84, 104, 256,  77, 117, 115, 105,  99,  32, 275])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"test_tok_folder/test_tokens_size280.npy\",\"rb\") as f:\n",
    "    a = np.load(f)\n",
    "a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
