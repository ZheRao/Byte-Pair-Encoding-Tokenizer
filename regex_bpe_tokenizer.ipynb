{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from typing import Optional\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import regex as re\n",
    "\n",
    "class ConfigError(Exception):\n",
    "    \"\"\"for invalid user configuration of tokenizer class\"\"\"\n",
    "    pass\n",
    "\n",
    "class regex_tokenizer:\n",
    "    \"\"\"\n",
    "        This is the base class for tokenizer, it provde basic functionalities such as produce pair counts and merge new tokens\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # initialize base vocabulary dictionary which is the character encoding based on UTF-8\n",
    "        self._base_vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self._base_vocab_size = 256\n",
    "        self.pattern = re.compile(r\"\"\" ?<\\|[a-z]+\\|>|'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    def _get_file_paths(self,title,vocab_size,tokenizer_folder_path=os.getcwd()):\n",
    "        self.folder = title + \"_regex_tok_folder\"\n",
    "        self.vocab_path = os.path.join(tokenizer_folder_path,self.folder, title + \"_vocab_dict_size\"+str(vocab_size)+\".pkl\")\n",
    "        self.merges_path = os.path.join(tokenizer_folder_path,self.folder, title + \"_merge_history_size\"+str(vocab_size)+\".pkl\")\n",
    "        self.tokens_path = os.path.join(tokenizer_folder_path,self.folder, title + \"_tokens_size\"+str(vocab_size)+\".txt\")\n",
    "\n",
    "    def _get_pair_counts(self,tokens, count_dict={}):\n",
    "        \"\"\"\n",
    "            treverse through the entire encoded text, produce a dictionary with paired occurrences of adjacent tokens\n",
    "                key: token pairs, e.g., (106, 32)\n",
    "                value: counts of occurrence of key, e.g., 300\n",
    "                meaning: token pair (106, 32) occurred 300 times in the text\n",
    "        \"\"\"\n",
    "        for (c1, c2) in zip(tokens[:-1],tokens[1:]):\n",
    "            count_dict[(c1,c2)] = count_dict.get((c1,c2),0) + 1\n",
    "        return count_dict\n",
    "    \n",
    "    def _merge_pair(self,tokens,pair,new_token):\n",
    "        \"\"\"\n",
    "            Replace all occurrences of pair in tokens by new_token\n",
    "        \"\"\"\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if i < len(tokens) - 1 and (tokens[i],tokens[i+1]) == pair:\n",
    "                new_tokens.append(new_token)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        return new_tokens\n",
    "    \n",
    "    def _recover_chunks(self,text):\n",
    "        \"\"\"\n",
    "            recovers converted nested list of integers back to nested list of integers\n",
    "        \"\"\"\n",
    "        recovered_chunks = [[int(num) for num in block.strip().split(\" \")] for block in text.split(\"\\n\")]\n",
    "        return recovered_chunks\n",
    "    \n",
    "    def _retrieve_training_history(self,title,vocab_size,tokenizer_folder_path=os.getcwd()):\n",
    "        \"\"\"\n",
    "            retrieve the dictionaries\n",
    "        \"\"\"\n",
    "        self._get_file_paths(title,vocab_size,tokenizer_folder_path)\n",
    "        try:\n",
    "            with open(self.vocab_path,\"rb\") as f:\n",
    "                vocab = pickle.load(f)\n",
    "            with open(self.merges_path,\"rb\") as f:\n",
    "                merges = pickle.load(f)\n",
    "            with open(self.tokens_path,\"r\") as f:\n",
    "                chunks = f.read()\n",
    "            token_chunks = self._recover_chunks(chunks)\n",
    "        except:\n",
    "            m = f\"Dictionary files do not exit, tokenizer requires training with {title} dataset. \\nOr provided with inconsistent vocab_size, use os.listdir to inspect dictionary files.\"\n",
    "            m_more = \" Or past_tokens cannot be retreived, if this is the case, encode text first\"\n",
    "            raise FileNotFoundError(m+m_more)\n",
    "        else:\n",
    "            return vocab, merges, token_chunks\n",
    "\n",
    "    def _initialize_special_tokens(self,token_list):\n",
    "        self.special_token_list = ['<|startofchapter|>']\n",
    "        if token_list is not None:\n",
    "            self.special_token_list.extend(token_list)\n",
    "        special_tokens_start_index = 100000\n",
    "        self.special_tokens = {}\n",
    "        for i in range(len(self.special_token_list)):\n",
    "            self.special_tokens[self.special_token_list[i]] = special_tokens_start_index+i\n",
    "        self.inverse_special_tokens = {v:k for k,v in self.special_tokens.items()}\n",
    "        self._get_file_paths(self.title,self.final_vocab_size)\n",
    "        self._save_special_tokens()\n",
    "    \n",
    "    def _save_special_tokens(self):\n",
    "        if not os.path.exists(self.folder):\n",
    "            os.makedirs(self.folder)\n",
    "        \n",
    "        with open(os.path.join(self.folder,self.title+\"_special_tokens.pkl\"), \"wb\") as f:\n",
    "            pickle.dump(self.special_tokens,f)\n",
    "    \n",
    "    def _read_update_special_tokens(self, token_list):\n",
    "        with open(os.path.join(self.folder,self.title+\"_special_tokens.pkl\"), \"rb\") as f:\n",
    "            self.special_tokens = pickle.load(f)\n",
    "        if token_list is not None:\n",
    "            special_token_list = list(self.special_tokens.keys())\n",
    "            difference = list(set(token_list) - set(special_token_list))\n",
    "            special_token_list.extend(difference)\n",
    "            self.special_token_list = special_token_list\n",
    "            max_idx = max(self.special_tokens.items(),key=lambda x:x[1])[1]\n",
    "            for i in range(len(difference)):\n",
    "                self.special_tokens[difference[i]] = max_idx + 1 + i\n",
    "        self.inverse_special_tokens = {v:k for k,v in self.special_tokens.items()}\n",
    "        self._save_special_tokens()\n",
    "\n",
    "class TrainTokenizer(regex_tokenizer):\n",
    "    \"\"\"\n",
    "        This class implement compression algorithm described in \n",
    "            https://en.wikipedia.org/wiki/Byte_pair_encoding#:~:text=Byte%20pair%20encoding%20(also%20known,for%20use%20in%20downstream%20modeling\n",
    "            with the addition of separating text into chunks before merging, avoid merging elements across categories: character with punctuation\n",
    "        It takes text and title, train a tokenizer and store the files in a directory\n",
    "        Args:\n",
    "            text: str, actual text\n",
    "            title: str, name of the mateirals that the tokenizer is training on\n",
    "            speical_token_list: list of special tokens besides <|startofchapter|>\n",
    "                must be in the form <|xxxxxx|>\n",
    "            fresh_start: bool, whether to train from scratch or continue training/compressing, default=True\n",
    "            final_vocab_size: int, final vocabulary size after compression - determines how many merges to perform, in thousands\n",
    "            last_vocab_size: int, if continue training, what is the last final_vocab_size in thousands: 10 -> 10,000\n",
    "        \n",
    "        Folder/Title Naming Convention:\n",
    "            \"book_title_base_tok_folder\"\n",
    "        \n",
    "        Sub-files (in the _tok_folder) Naming Convetion:\n",
    "            \"book_title_vocab_dict_size10.pkl\" stores encoding dictionary, where 10 means 10,000 vocabulary size\n",
    "            \"book_title_merge_history_size10.pkl\" stores merging history, where 10 means 10,000 vocabulary size\n",
    "            \"book_title_tokens_size10.npy\" stores tokens from last compression, with tokenization with size 10,000\n",
    "            \"book_title_special_tokens.pkl\" stores dictionary for special tokens\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text: str, title: str, special_token_list=None, final_vocab_size: int =6000, fresh_start: bool =True, last_vocab_size: Optional[int] =None):\n",
    "        super(TrainTokenizer,self).__init__()\n",
    "        self.title = title\n",
    "        self.final_vocab_size = final_vocab_size\n",
    "        # initialize training vocabulary and merge history dictionaries\n",
    "        if fresh_start:\n",
    "            self._initialize_special_tokens(special_token_list)\n",
    "            self.vocab = self._base_vocab\n",
    "            self.merge_history = {}\n",
    "            for token in self.special_token_list:\n",
    "                text = text.replace(token,\"\")\n",
    "            text_chunks = re.findall(self.pattern,text)\n",
    "            self.token_chunks = [list(chunk.encode(\"utf-8\")) for chunk in text_chunks]\n",
    "        else:\n",
    "            if last_vocab_size is None: raise ConfigError(\"for continue training (fresh_start == False), last final_vocab_size must be provided\")\n",
    "            if final_vocab_size <= last_vocab_size: raise ConfigError(\"unable to perform tokenizer training, because new vocabulary size must be larger than previous vocabulary size\")\n",
    "            self.vocab, self.merge_history, self.token_chunks = self._retrieve_training_history(self.title,last_vocab_size)\n",
    "            self._read_update_special_tokens(special_token_list)\n",
    "        assert len(self.vocab) == (len(self.merge_history) + self._base_vocab_size), \"dictionary lengths not matching - the following should be true: voca = merge_hist + 256\"\n",
    "        assert final_vocab_size > len(self.vocab), f\"final vocabulary size specified is too small, must be larger than {len(self.vocab)}\"\n",
    "\n",
    "        \n",
    "    \n",
    "    def _process_token_chunks(self,token_chunks):\n",
    "        \"\"\"\n",
    "            make [[1,2,3], [4,5,6,7], [8,9,10]] into string: \n",
    "                1 2 3\n",
    "                 4 5 6 7\n",
    "                 8 9 10\n",
    "        \"\"\"\n",
    "        chunks_str = str(token_chunks)\n",
    "        chunks_str = chunks_str[1:-2] # removing leading \"[\" and trailing \"]]\"\n",
    "        chunks_str = chunks_str.replace(\"[\", \"\") # remove all \"[\"\n",
    "        chunks_str = chunks_str.replace(\"],\", \"\\n\") # replace end of each list to be a new line character\n",
    "        chunks_str = chunks_str.replace(\",\", \"\") # remove all comma\n",
    "        return chunks_str\n",
    "\n",
    "    \n",
    "    def _perform_merge(self):\n",
    "        \"\"\"\n",
    "            Training loop compression process:\n",
    "                1. identify top pair\n",
    "                2. swap the occurrences of top pair in each token chunk\n",
    "                3. update merge_history and vocab\n",
    "            the training loop ignores special characters\n",
    "            after training, vocab, merge_history are saved as pickle files and final tokens are saved as npy file\n",
    "        \"\"\"\n",
    "        vocab_size = len(self.merge_history) + self._base_vocab_size\n",
    "        num_merges = self.final_vocab_size - vocab_size\n",
    "        progress_bar = tqdm(range(num_merges))\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            progress_bar.update(1)\n",
    "            pair_counts = {}\n",
    "            for chunk in self.token_chunks:\n",
    "                pair_counts = self._get_pair_counts(tokens=chunk,count_dict=pair_counts)\n",
    "            top_pair = max(pair_counts,key=pair_counts.get)\n",
    "            self.token_chunks = [self._merge_pair(tokens=chunk,pair=top_pair,new_token=vocab_size + i) for chunk in self.token_chunks]\n",
    "            self.merge_history[top_pair] = vocab_size+i\n",
    "            self.vocab[vocab_size+i] = self.vocab[top_pair[0]] + self.vocab[top_pair[1]]\n",
    "            print(f\"merged {top_pair} as {vocab_size+i}\")\n",
    "        \n",
    "        self._get_file_paths(self.title,self.final_vocab_size)\n",
    "\n",
    "        if not os.path.exists(self.folder):\n",
    "            os.makedirs(self.folder)\n",
    "        \n",
    "        # save files to folder\n",
    "        with open(self.vocab_path,\"wb\") as f:\n",
    "            pickle.dump(self.vocab,f)\n",
    "        with open(self.merges_path,\"wb\") as f:\n",
    "            pickle.dump(self.merge_history, f)\n",
    "        processed_token_chunks = self._process_token_chunks(self.token_chunks)\n",
    "        with open(self.tokens_path,\"w\") as f:\n",
    "            f.write(processed_token_chunks)\n",
    "    \n",
    "    def run(self):\n",
    "        self._perform_merge()\n",
    "\n",
    "class ApplyTokenizer(regex_tokenizer):\n",
    "    \"\"\"\n",
    "        This subclass can encode and decode text based on trained tokenizer\n",
    "        Arg:\n",
    "            mode: str, \"encode\" or \"decode\"\n",
    "            title: str, tokenizer trained on which texts user wish to apply\n",
    "            vocab_size: int, which version of the tokenizer user wish to apply, usually in thousands\n",
    "            tokenizer_folder_path: file path to the tokenizer folder, to access the dictionaries\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, title, vocab_size, tokenizer_folder_path):\n",
    "        super(ApplyTokenizer,self).__init__()\n",
    "        self.vocab, self.merge_history, _ = self._retrieve_training_history(title=title,vocab_size=vocab_size,tokenizer_folder_path=tokenizer_folder_path)\n",
    "    \n",
    "    def encode(self,text):\n",
    "        assert type(text) == str, \"input for encoding is not string\"\n",
    "        tokens = list(text.encode(\"utf-8\"))\n",
    "        while len(tokens) >= 2:\n",
    "            pairs = self._get_pair_counts(tokens=tokens) # dictionary of (101, 102) 30000\n",
    "            pair_replace = min(pairs,key=lambda x: self.merge_history.get(x,float('inf'))) # if no match, returns itself\n",
    "            if pair_replace not in self.merge_history:\n",
    "                break\n",
    "            new_token = self.merge_history[pair_replace]\n",
    "            tokens = self._merge_pair(tokens=tokens,pair=pair_replace,new_token=new_token)\n",
    "        return tokens\n",
    "\n",
    "    def decode(self,tokens):\n",
    "        text = b\"\".join([self.vocab[i] for i in tokens])\n",
    "        text_decoded = text.decode(\"utf-8\",errors=\"replace\")\n",
    "        return text_decoded\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length of the books is 3729707\n"
     ]
    }
   ],
   "source": [
    "with open(\"lord-of-the-rings-processed.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "print(f\"Total length of the books is {len(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = \"<|startofchapter|>\" + text + \"<|end|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = TrainTokenizer(text=text2,title=\"test\",final_vocab_size=258,special_token_list=[\"<|end|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26c2746eecb42fca4c841dd650cfff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged (32, 116) as 256\n",
      "merged (104, 101) as 257\n"
     ]
    }
   ],
   "source": [
    "tok.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 115, 97, 105, 100], [46], [32]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.token_chunks[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok2 = TrainTokenizer(text=\" \", title=\"test\",final_vocab_size=260,fresh_start=False,last_vocab_size=258,special_token_list=[\"<|end|>\",\"<|start|>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbc7039cb2b748678b8c481f3e790c17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merged (32, 97) as 258\n",
      "merged (256, 257) as 259\n"
     ]
    }
   ],
   "source": [
    "tok2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[87, 101, 108, 108],\n",
       " [44],\n",
       " [32, 73],\n",
       " [39, 109],\n",
       " [32, 98, 97, 99, 107],\n",
       " [44, 39],\n",
       " [32, 257],\n",
       " [32, 115, 97, 105, 100],\n",
       " [46],\n",
       " [32]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2.token_chunks[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100000: '<|startofchapter|>', 100001: '<|end|>', 100002: '<|start|>'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok2.inverse_special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
